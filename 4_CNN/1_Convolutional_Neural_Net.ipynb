{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Net(합성곱 신경망, CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:20:51.579030Z",
     "start_time": "2018-10-17T12:20:51.565637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module): \n",
    "    # 일반적으로 class를 이용해 neural net 모형을 정의합니다.\n",
    "    # class의 개념에 익숙하지 않으신 분들은 https://wikidocs.net/28 여기를 참고하세요.\n",
    "    def __init__ (self):\n",
    "        super(Net, self).__init__() #super는 슈퍼 클래스의 method를 호출합니다.\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        \n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1,6,5) # 1,6,5는 각각 in_channels, out_channels, kernel_size를 뜻합니다. filter 6개를 씁니다.\n",
    "        self.conv2 = nn.Conv2d(6,16,5) # 6으로 나왔으니 6으로 받고 16으로 나갑니다, kernel_size는 5입니다.\n",
    "        #kernel_size는 convolving하는 kernel의 크기를 뜻합니다. kernel = filter입니다. 여기서는 5x5 겠죠?\n",
    "        \n",
    "        # an affine operation: y = Wx +b\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        # Max pooling over a (2,2) window\n",
    "        x = F.max_pool2d(x,(2,2)) #input = x, kernel_size = 2x2, stride = None, stirde=None이면 kernel_size를 따라갑니다. stride=2\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        \n",
    "        # -1은 몇개가 들어올 지 모른다는 뜻입니다.\n",
    "        # num_flat_features는 1x? 형태로 flat하게 reshape합니다.\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    # x.size()[0]은 input 갯수이기 때문에 제외합니다. 각각의 input값들을 1x?형태로 reshape합니다.\n",
    "    # torch.randn(5,3,3) 을 생각해보면 개수를 의미하는 5를 제외하고 3x3 = 9의 값을 return합니다.\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward 이후에 backward는 autograd를 이용해 자동의로 정의할 수 있습니다.\n",
    "- net의 weight들은 net.parameters()에 의해 return됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:20:52.729371Z",
     "start_time": "2018-10-17T12:20:52.723420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) #conv1의 weight 벡터 사이즈입니다. 5x5 행렬 6개겠죠? print(params)로 확인해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:20:53.340079Z",
     "start_time": "2018-10-17T12:20:53.330519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0367, -0.0806,  0.0891, -0.0654, -0.0138, -0.0105, -0.0813, -0.0589,\n",
      "         -0.0416,  0.1057]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32) #32x32 하나를 input으로 넣어줍니다. 차원에 유의하세요.\n",
    "\n",
    "out = net(input) \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input이 왜 1x1x32x32로 들어갈까요? 이유는 간단합니다. torch.nn에서는 mini-batch만 지원합니다.  \n",
    "따라서 nnconv2d는 nsamples x nChannels x Height x Width의 4차원 tensor를 입력으로 합니다.  \n",
    "1개의 값을 input channel(1)에 넣으니 1x1이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:20:54.319405Z",
     "start_time": "2018-10-17T12:20:54.310477Z"
    }
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10)) # loss를 정의하지 않았으므로 임의의 값으로 back prop을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor는 autograd 연산을 지원합니다. 또한 tensor의 gradient를 갖고 있습니다. back prop할때마다 이를 초기화 해야 합니다.  \n",
    "nn.module은 weight를 캡슐화해서 GPU로의 이동, 내보내기, 불러오기 등의 작업을 도와줍니다.  \n",
    "nn.parameter는 tensor의 종류로 module에 할당될 때 자동으로 weight로 등록됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function(손실 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수는 output, target을 한 쌍의 입력으로 받아 ouput이 target으로부터 얼마나 떨어져 있는지를 추정하는 값을 계산합니다.  \n",
    "output은 net이 계산한 추정값(출력), target은 실제 값(정답)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:20:55.626487Z",
     "start_time": "2018-10-17T12:20:55.613622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.6260, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.arange(1,11, dtype = torch.float)\n",
    "target = target.view(1,-1) # reshape same as torch.unsqueeze(target, dim=0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:01:01.630285Z",
     "start_time": "2018-10-17T12:01:01.623839Z"
    }
   },
   "source": [
    "loss.grad_fn는 다음과 같은 연산 그래프를 따릅니다.  \n",
    "\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n",
    "-> view -> linear -> relu -> linear -> relu -> linear  \n",
    "-> MSELoss  \n",
    "-> loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:20:56.748888Z",
     "start_time": "2018-10-17T12:20:56.738970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000024F41EF1CC0>\n",
      "<ThAddmmBackward object at 0x0000024F41EF15C0>\n",
      "<ReluBackward object at 0x0000024F41F25048>\n",
      "<ThAddmmBackward object at 0x0000024F41EF15C0>\n",
      "<ReluBackward object at 0x0000024F41F25048>\n",
      "<ThAddmmBackward object at 0x0000024F41EF1CC0>\n",
      "<ViewBackward object at 0x0000024F41F25048>\n",
      "<MaxPool2DWithIndicesBackward object at 0x0000024F41EF16D8>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0]) #Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0]) #relu\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0].next_functions[0][0]) #Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0].next_functions[0][0].next_functions[1][0]) #relu\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0].next_functions[0][0].next_functions[1][0].next_functions[0][0]) #Linear\n",
    "a = loss.grad_fn.next_functions[0][0].next_functions[1][0].next_functions[0][0].next_functions[1][0].next_functions[0][0]\n",
    "print(a.next_functions[1][0]) #view\n",
    "print(a.next_functions[1][0].next_functions[0][0]) #maxpool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:22:55.662841Z",
     "start_time": "2018-10-17T12:22:55.641513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0080, -0.0462,  0.0108,  0.0174,  0.0168, -0.0126])\n",
      "tensor([ 0.0803,  0.0054, -0.1227, -0.0187, -0.0357,  0.0007,  0.0455, -0.0923,\n",
      "         0.0324,  0.1421,  0.0000, -0.0450,  0.0514,  0.0545, -0.0093, -0.0722])\n",
      "tensor([ 0.0807,  0.0518, -0.0182,  0.0000,  0.0000,  0.0462, -0.1344,  0.0527,\n",
      "         0.0000,  0.0000,  0.0686,  0.1070,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.0323, -0.1667, -0.1134,  0.0000,  0.0046,  0.0000,  0.0000,\n",
      "         0.0404,  0.0000,  0.0000,  0.0000,  0.0000, -0.0142,  0.0000,  0.0655,\n",
      "         0.0000,  0.0000,  0.0596, -0.0923,  0.0000,  0.0000,  0.0000, -0.0731,\n",
      "         0.0239,  0.0898,  0.0051,  0.0000,  0.0000,  0.0000, -0.0244,  0.0000,\n",
      "         0.0000,  0.0000,  0.0094,  0.1050,  0.0000,  0.1050,  0.0000,  0.1596,\n",
      "         0.0003, -0.1304, -0.0382, -0.0671,  0.0000,  0.0000,  0.0000, -0.0119,\n",
      "         0.0000,  0.0000, -0.0717,  0.0000,  0.0167, -0.0732,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0953,  0.0000,  0.0000, -0.0871,\n",
      "         0.0000, -0.0127,  0.0050, -0.0637,  0.0000, -0.0107,  0.0000,  0.0000,\n",
      "         0.0185,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0839,  0.0416,\n",
      "         0.0742,  0.0000, -0.1479,  0.0000, -0.0184,  0.0000,  0.0000,  0.0000,\n",
      "         0.0350,  0.0551,  0.0000,  0.0000, -0.0520,  0.0000, -0.0500,  0.0000,\n",
      "        -0.1769,  0.0000,  0.0000,  0.0000, -0.0015,  0.0000,  0.1503,  0.0196])\n",
      "tensor([-0.0453,  0.0000,  0.0000,  0.0237, -0.1493,  0.0000,  0.0000,  0.2050,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.2335, -0.4697,  0.3458,  0.0000,\n",
      "         0.5370,  0.0000, -0.0915,  0.0000,  0.4379, -0.3769,  0.1271,  0.0000,\n",
      "         0.0000, -0.1310,  0.2106, -0.0894,  0.0000, -0.3154,  0.0000,  0.0000,\n",
      "        -0.1814,  0.4204,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1060,\n",
      "         0.0000,  0.0000,  0.0000,  0.2240, -0.1074, -0.3149,  0.0000,  0.1450,\n",
      "         0.0000,  0.1239,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000,  0.0000,\n",
      "        -0.5238,  0.0181,  0.2978,  0.0000, -0.2415, -0.1514, -0.3142, -0.0692,\n",
      "         0.0000,  0.0350,  0.4466,  0.0000,  0.5822, -0.0699, -0.0855,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.1720,  0.0000,  0.0000,  0.2326,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000])\n",
      "tensor([-0.1927, -0.4161, -0.5822, -0.8131, -1.0028, -1.2021, -1.4163, -1.6118,\n",
      "        -1.8083, -1.9789])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad() # 각 연산의 gradient가 저장되어 있는데 그 값을 초기화 해줍니다.\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "# 보다시피 값이 모두 0으로 초기화되어 있습니다.\n",
    "\n",
    "loss.backward() # gradient를 계산합니다.\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n",
    "print(net.conv2.bias.grad)\n",
    "print(net.fc1.bias.grad)\n",
    "print(net.fc2.bias.grad)\n",
    "print(net.fc3.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight update(가중치 갱신)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 갱신은 torch.optim 패키지를 이용해 이루어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:27:53.414388Z",
     "start_time": "2018-10-17T12:27:53.386764Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer를 생성합니다\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.01) #Adam optimizer를 사용합니다. learning rate는 0.01입니다.\n",
    "\n",
    "# 학습 과정(가중치 갱신 과정)은 다음과 같습니다.\n",
    "optimizer.zero_grad() # 기존의 변화도에 대해 누적되는 것을 막기 위해 zero_grad()로 초기화합니다.\n",
    "output = net(input)\n",
    "loss = criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
